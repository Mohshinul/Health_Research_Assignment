---
title: "Assignment2"
author: "Mohshinul Karim"
date: "2024-05-30"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tidytext)
library(textstem)
library(clinspacy)
library(topicmodels)
library('reshape2')
library(stringr)

```

## Data Parsing

After that we can grab the dataset directly from the `clinspacy` library.

```{r}
raw.data <- clinspacy::dataset_mtsamples()
dplyr::glimpse(raw.data)
```

There is no explanation or data dictionary with this dataset, which is a surprisingly common and frustrating turn of events!  

### Q1. Using the output of dplyr's `glimpse` command (or rstudio's data viewer by clicking on `raw.data` in the Environment pane) provide a description of what you think each variable in this dataset contains.

### Answer1: 
Possible description of each variable in the raw.data dataset:

**note_id (<int>):** This variable contains unique integer identifiers for each note or record in the dataset. 

**description (<chr>):** This variable contains a short textual description of the patient case, which may includes a brief overview of the patient's demographic information (e.g., age, gender, ethnicity).

**medical_specialty (<chr>):** This variable contains the medical specialty or specialties relevant to the note. Examples include "Allergy / Immunology", "Bariatrics", and "Cardiology". It indicates the field of medicine that pertains to the patient's condition or treatment.

**sample_name (<chr>):** This variable contains the name or title of the sample case or medical condition described in the note. It provides a more specific identifier for the type of medical case, such as "Allergic Rhinitis" or "Laparoscopic Gastric Bypass Consultation".

**transcription (<chr>):** This variable contains the full text transcription of the medical note or report. It includes detailed information about the patient's symptoms, history, examination findings, diagnosis, and treatment plan. The text is typically in a structured format, often starting with sections like "SUBJECTIVE" to describe patient-reported symptoms.

**keywords (<chr>):** This variable contains a list of keywords or key phrases relevant to the medical note. These keywords are likely extracted to facilitate searching and categorization of the notes. They summarize the main topics or conditions discussed in the note, such as "allergy / immunology", "allergic rhinitis", and "allergies".


Let's see how many different medical specialties are featured in these notes: 
```{r}
raw.data %>% dplyr::select(medical_specialty) %>% dplyr::n_distinct()
```

So, how many transcripts are there from each specialty:

```{r}
ggplot2::ggplot(raw.data, ggplot2::aes(y=medical_specialty)) + ggplot2::geom_bar() + labs(x="Document Count", y="Medical Speciality" )
```
```{r}
# Organizing the chart from lower to higher
ggplot2::ggplot(raw.data, ggplot2::aes(y = reorder(medical_specialty, -table(medical_specialty)[medical_specialty]))) + 
  ggplot2::geom_bar() + 
  labs(x = "Document Count", y = "Medical Specialty") +
  theme_minimal() 
```

Let's make our life easier and filter down to 3 specialties: a diagonstic/lab, a medical, and a surgical specialty

```{r} 
filtered.data <- raw.data %>% dplyr::filter(medical_specialty %in% c("Orthopedic", "Radiology", "Surgery")) 
```

## Text Processing

Let's now apply our standard pre-processing to the transcripts from these specialties.  
We are going to use the `tidytext` package to tokenise the transcript free-text.  
Let's remove stop words first. e.g., "the", "of", "to", and so forth. These are known as stop words and we can remove them relative easily using a list from  `tidytext::stop_words` and `dplyr::anti_join()`

```{r}
analysis.data <- filtered.data %>%
  unnest_tokens(word, transcription) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %>%
  filter(!str_detect(word, "[0-9]")) %>%
  anti_join(stop_words) %>%
  group_by(note_id) %>%
  summarise(transcription = paste(word, collapse = " ")) %>%
  left_join(select(filtered.data, -transcription), by = "note_id")
```

Now let's tokenize the `transcription` to words (unigram) 
By default this tokenises to words but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regular expression.

```{r}
tokenized.data.unigram <- analysis.data %>% tidytext::unnest_tokens(word, transcription, to_lower=TRUE)
```

You can also do bi-grams
```{r}
tokenized.data <- analysis.data %>% tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n=2, to_lower = TRUE)
```

How many stop words are there in `tidytext::stop_words` from each lexicon?
```{r}
tidytext::stop_words %>% dplyr::group_by(lexicon) %>% dplyr::distinct(word) %>% dplyr::summarise(n=dplyr::n())
```

### Q2. How many unique unigrams are there in the transcripts from each specialty:

### Answer2: 
The number of unique unigrams from each speciality are shown in the following tibble:

```{r}
# Count unique unigrams by specialty
unique_unigrams_by_specialty <- tokenized.data.unigram %>%
  group_by(medical_specialty) %>%
  distinct(word) %>%
  summarise(n = n())

print(unique_unigrams_by_specialty)
```

Let's plot some distribution of unigram tokens (words)

```{r}
word_counts <- tokenized.data.unigram %>%
    group_by(word) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Words",
       y = "Number of Words")
```

Let's plot some distribution of bigram tokens (words)

```{r}
word_counts <- tokenized.data %>%
    group_by(ngram) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Bigrams",
       y = "Number of Words")
```

### Q3. How many unique bi-grams are there in each category without stop words and numbers?

### Answer3: 

The numbers are shown in the tibble:
```{r}
#Step 1: Filter out stop words and numbers from bi-grams
# Separate the bi-grams into individual words
tokenized_bigrams <- tokenized.data %>%
  separate(ngram, into = c("word1", "word2"), sep = " ")

# Remove stop words and numbers
filtered_bigrams <- tokenized_bigrams %>%
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
  filter(!str_detect(word1, "[0-9]"), !str_detect(word2, "[0-9]"))

# Reconstruct the bi-grams
filtered_bigrams <- filtered_bigrams %>%
  mutate(bigram = paste(word1, word2, sep = " ")) %>%
  select(note_id, medical_specialty, bigram)

```

```{r}
#Step 2: Count unique bi-grams by specialty
# Count unique bi-grams by medical specialty
unique_bigrams_by_specialty <- filtered_bigrams %>%
  group_by(medical_specialty) %>%
  distinct(bigram) %>%
  summarise(n = n())

print(unique_bigrams_by_specialty)

```

Sometimes we are interested in tokenising/segmenting things other than words like whole sentences or paragraphs.  
### Q4. How many unique sentences are there in each category? Hint: use `?tidytext::unnest_tokens` to see the documentation for this function.

### Answer4:

```{r}
#Step 1: Tokenize the transcriptions into sentences
# Tokenize the transcription column into sentences
tokenized_sentences <- analysis.data %>%
  tidytext::unnest_tokens(sentence, transcription, token = "sentences", to_lower = FALSE)

```

```{r}
#Step 2: Count unique sentences by specialty
# Count unique sentences by medical specialty
unique_sentences_by_specialty <- tokenized_sentences %>%
  group_by(medical_specialty) %>%
  distinct(sentence) %>%
  summarise(n = n())

print(unique_sentences_by_specialty)

```

Now that we've tokenized to words and removed stop words, we can find the most commonly word used within each category:

```{r}
tokenized.data %>%
  dplyr::group_by(medical_specialty) %>%
  dplyr::count(ngram, sort = TRUE) %>%
  dplyr::top_n(5)
```

We should lemmatize the tokenized words to prevent over counting of similar words before further analyses.  
Annoyingly, `tidytext` doesn't have a built-in lemmatizer.

### Q5. Do you think a general purpose lemmatizer will work well for medical data? Why might it not?

### Answer5:
A general-purpose lemmatizer might not work well for medical data because it is not specifically trained to understand and process medical terminology. Here are a few reasons why:
**Specialized Vocabulary:** Medical data contains a large number of specialized terms and jargon that are unique to the field of medicine which may be unrecognized by A general-purpose lemmatizer.

**Acronyms and Abbreviations:** The medical field uses many acronyms and abbreviations (For instance, "BP" for blood pressure or "MRI" for magnetic resonance imaging) that a general-purpose lemmatizer may not handle correctly.

**Context Sensitivity:** Medical terms can have different meanings depending on the context. For example, "lead" in a medical context could refer to an ECG lead, which is different from the common meaning of the word. 

**Accuracy and Specificity:** Medical lemmatization requires a high degree of accuracy and specificity. Which is not the case of general lemmatization.


Unfortunately, a specialised lemmatizer like in `clinspacy` is going to be very painful to install so we will just use a simple lemmatizer for now:

```{r}
lemmatized.data <- tokenized.data %>% dplyr::mutate(lemma=textstem::lemmatize_words(ngram))
```

We can now calculate the frequency of lemmas within each specialty and note.
```{r}
lemma.freq <- lemmatized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion) %>%
  tidyr::pivot_longer(`Surgery`:`Radiology`,
               names_to = "medical_specialty", values_to = "proportion")
```

And plot the relative proportions 
```{r}

ggplot2::ggplot(lemma.freq, ggplot2::aes(x=proportion, 
                                         y=`Orthopedic`,
                                         color=abs(`Orthopedic` - proportion))) + 
  ggplot2::geom_abline(color="gray40", lty=2) +
  ggplot2::geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
  ggplot2::geom_text(ggplot2::aes(label=lemma), check_overlap=TRUE, vjust=1.5) +
  ggplot2::scale_x_log10(labels=scales::percent_format()) + 
  ggplot2::scale_y_log10(labels=scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits=c(0, 0.001), low="red", high="gray75") +
  ggplot2::facet_wrap(~medical_specialty, ncol = 2) +
  ggplot2::theme(legend.position="none") +
  ggplot2:: labs(y="Orthopedic", x = NULL)
```

### Q6. What does this plot tell you about the relative similarity of lemma frequencies between Surgery and Orthopedic and between radiology and Orthomedic? Based on what these specialties involve, is this what you would expect?

### Answer6: 
**Analysis Based on the Plot:** The plot shows the distribution of lemma frequencies for Surgery (right panel) and Radiology (left panel) against Orthopedic (both y-axes).
**1.	Relative Similarity Between Surgery and Orthopedic:** For Surgery and Orthopedic, there is a noticeable cluster of lemmas that align closely along the diagonal line, indicating a high level of similarity in the frequency of these terms between the two specialties. The larger the dots and the closer they are to the diagonal line, the more similar the lemma frequencies are between Surgery and Orthopedic. Terms like "carpal ligament," "fracture site," and "longitudinal ligament" appear frequently in both Surgery and Orthopedic notes, which makes sense given that both specialties deal heavily with musculoskeletal issues and surgical procedures.
**2.	Relative Similarity Between Radiology and Orthopedic:** In the Radiology and Orthopedic comparison (left panel), there is also a cluster of lemmas along the diagonal line, but it is less dense compared to Surgery and Orthopedic. Terms such as "series images," "acute fracture," and "abnormal" appear frequently in both Radiology and Orthopedic, but the overall spread indicates more variability. This is because Radiology deals with imaging across a variety of conditions and specialties, whereas Orthopedic is more focused on the musculoskeletal system.
**Expectations Based on Specialties:**
**•	Surgery and Orthopedic:** It is expected that Surgery and Orthopedic have a high overlap in terminology because both fields often involve surgical intervention on bones, joints, and related structures. Terms specific to procedures, ligaments, and fractures are common in both.
**•	Radiology and Orthopedic:** While Radiology supports Orthopedic by providing crucial imaging for diagnosis and treatment, it also covers a broader range of conditions beyond Orthopedic issues. Therefore, some overlap in terms is expected, but Radiology will also have unique terms related to imaging technology and procedures not specific to Orthopedic. 


### Q7. Modify the above plotting code to do a direct comparison of Surgery and Radiology (i.e., have Surgery or Radiology on the Y-axis and the other 2 specialties as the X facets)

### Answer7:

```{r}
lemma.freq2 <- lemmatized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion) %>%
  tidyr::pivot_longer(`Orthopedic`:`Radiology`,
               names_to = "medical_specialty", values_to = "proportion")
```


```{r}
# Plot the direct comparison of Surgery and Radiology with Surgery on the Y-axis
ggplot2::ggplot(lemma.freq2, ggplot2::aes(x=proportion, 
                                         y=`Surgery`,
                                         color=abs(`Surgery` - proportion))) + 
  ggplot2::geom_abline(color="gray40", lty=2) +
  ggplot2::geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
  ggplot2::geom_text(ggplot2::aes(label=lemma), check_overlap=TRUE, vjust=1.5) +
  ggplot2::scale_x_log10(labels=scales::percent_format()) + 
  ggplot2::scale_y_log10(labels=scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits=c(0, 0.001), low="green", high="gray75") +
  ggplot2::facet_wrap(~medical_specialty, ncol = 2) +
  ggplot2::theme(legend.position="none") +
  ggplot2::labs(y="Surgery", x = NULL)

```

**Analysis Based on the Plot:** From the graph, we can compare the lemma frequencies between Surgery and Radiology. The terms associated with Surgery have a more even spread across a wide range of frequencies, indicating a diverse set of commonly used terms. In contrast, the terms for Radiology are more clustered, suggesting that certain terms are used much more frequently than others. The diagonal line represents a perfect correlation between the lemma frequencies of Surgery and Radiology. The spread of terms around this line shows that while there is some overlap in terminology, the terms frequently used in Surgery and Radiology tend to be distinct, reflecting the different focuses of these specialties.
Overall, the graph highlights the differences in common terminology between Surgery and Radiology, with Surgery having a broader range of frequent terms related to procedures and body parts, and Radiology focusing more on diagnostic terms. This distinction is expected given the nature of these medical specialties.

## TF-IDF Normalisation

Maybe looking at lemmas across all notes in a specialty is misleading, what if we look at lemma frequencies across a specialty.

```{r}
lemma.counts <- lemmatized.data %>% dplyr::count(medical_specialty, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(medical_specialty) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)
```
Now we can calculate the term frequency / invariant document frequency (tf-idf):

```{r}
all.counts.tfidf <- tidytext::bind_tf_idf(all.counts, lemma, medical_specialty, n) 
```

We can then look at the top 10 lemma by tf-idf within each specialty:
  
```{r}
all.counts.tfidf %>% dplyr::group_by(medical_specialty) %>% dplyr::slice_max(order_by=tf_idf, n=10)
```
### Q8: Are there any lemmas that stand out in these lists? Why or why not?

### Answer8:
Looking at the lists of lemmas for each medical specialty, Not any single lemmas stand out in all the three lists. But for all three speciality there are difffernt lemma that stand out.

**For Orthopedic:** range motion stands out.
**For Radiology:** left ventricular and ejection fraction stand out. 
**For Surgery:** closed vicryl, anesthesia endotracheal, endotracheal anesthesia,	steri strips, and dissection carried stand out.

These lemmas stand out because they are highly relevant to the respective medical specialties, indicating that the terms are frequently used in their specific contexts. This specificity helps ensure that the terms are unique to the disciplines they represent, thus providing clear differentiation between specialties. Ans that's why there is not any common lemma for all three to stands out. 

We can look at transcriptions in full using these lemmas to check how they are used with `stringr::str_detect`
```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>% dplyr::filter(stringr::str_detect(transcription, 'steri strips')) %>% dplyr::slice(1)
```

### Q9. Extract an example of one of the other "top lemmas" by modifying the above code

### Answer9: 
```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>%
  dplyr::filter(stringr::str_detect(transcription, 'carpal ligament')) %>% dplyr::slice(1)

```


## Topic Modelling

In NLP, we often have collections of documents (in our case EMR transcriptions) that we’d like to divide into groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.


- Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”


- Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.

First lets calculate a term frequency matrix for each transcription:
```{r}

lemma.counts <- lemmatized.data %>% dplyr::count(note_id, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(note_id) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)

emr.dcm <- all.counts %>% tidytext::cast_dtm(note_id, lemma, n)
```

Then we can use LDA function to fit a 5 topic (`k=5`) LDA-model
```{r}
emr.lda <- topicmodels::LDA(emr.dcm, k=5, control=list(seed=42))
emr.topics <- tidytext::tidy(emr.lda, matrix='beta')
```

Then we can extract the top terms per assigned topic:
```{r}

top.terms <- emr.topics %>% dplyr::group_by(topic) %>% 
  dplyr::slice_max(beta, n=10) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)


top.terms %>% 
  dplyr::mutate(term=tidytext::reorder_within(term, beta, topic)) %>% 
  ggplot2::ggplot(ggplot2::aes(beta, term, fill=factor(topic))) + 
    ggplot2::geom_col(show.legend=FALSE) + 
    ggplot2::facet_wrap(~ topic, scales='free')  +
    ggplot2::theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1)) +
    tidytext::scale_y_reordered()
```



Now we can ask how well do these assigned topics match up to the medical specialties from which each of these transcripts was derived.

```{r}
specialty_gamma <- tidytext::tidy(emr.lda, matrix='gamma')

# we need to join in the specialty from the note_id
note_id_specialty_mapping <- lemmatized.data %>%
  dplyr::mutate(document=as.character(note_id)) %>% 
  dplyr::select(document, medical_specialty) %>% 
  dplyr::distinct()

specialty_gamma <- dplyr::left_join(specialty_gamma, note_id_specialty_mapping)
```

```{r}

specialty_gamma %>%
  dplyr::mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ medical_specialty) +
  ggplot2::labs(x = "topic", y = expression(gamma))
```

Interestingly, Surgery, Orthopedic, and Radiology assign mostly to a single topics. We'd possibly expect this from radiology due to referring to imaging for many different diagnoses/reasons. 
However, this may all just reflect we are using too few topics in our LDA to capture the range of possible assignments. 

### Q10 Repeat this with a 6 topic LDA, do the top terms from the 3 topic LDA still turn up? How do the specialties get split into sub-topics?

### Answer:
We can use LDA function to fit a 6 topic (`k=6`) LDA-model
```{r}
emr.lda6 <- topicmodels::LDA(emr.dcm, k=6, control=list(seed=42))
emr.topics6 <- tidytext::tidy(emr.lda6, matrix='beta')
```

Then we can extract the top terms per assigned topic:
```{r}

top.terms6 <- emr.topics6 %>% dplyr::group_by(topic) %>% 
  dplyr::slice_max(beta, n=10) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)


top.terms6 %>% 
  dplyr::mutate(term=tidytext::reorder_within(term, beta, topic)) %>% 
  ggplot2::ggplot(ggplot2::aes(beta, term, fill=factor(topic))) + 
    ggplot2::geom_col(show.legend=FALSE) + 
    ggplot2::facet_wrap(~ topic, scales='free')  +
    ggplot2::theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1)) +
    tidytext::scale_y_reordered()
```
**Analysis:** Terms like "prepped draped," "preoperative diagnosis," "procedure patient," "tolerated procedure," and "blood loss" are consistently present across both models, indicating their high relevance.
In the 6 topic LDA, these terms are spread across more specific topics, reflecting a finer granularity in the topic modeling. This finer distribution helps to identify more specific clusters within the data, while still recognizing the most significant terms identified in the 5 topic LDA.

**The specialties get split into sub-topics:**
From the 6 topic LDA, the specialties get split into more specific sub-topics:

**Orthopedic:** Primarily associated with terms like "prepped draped," "carpal tunnel," "carpal ligament," "range motion," "lower extremity," indicating specific orthopedic procedures and conditions.
**Surgery:** Terms such as "prepped draped," "preoperative diagnosis," "procedure patient," "postoperative diagnosis," and "tolerated procedure" are common across multiple sub-topics, reflecting the different stages and types of surgical procedures.
**Radiology:** Keywords like "coronary artery," "pulmonary artery," "left anterior," "anterior descending," and "left ventricular" are notable, indicating a focus on imaging and diagnostics related to cardiovascular and pulmonary systems.

The 6 topic LDA thus provides a more detailed sub-topic analysis, helping to better understand the nuances within each specialty.



##################################################


**Extra**
Now we can ask how well do these assigned topics match up to the medical specialties from which each of these transcripts was derived.

```{r}
specialty_gamma6 <- tidytext::tidy(emr.lda6, matrix='gamma')

# we need to join in the specialty from the note_id
note_id_specialty_mapping6 <- lemmatized.data %>%
  dplyr::mutate(document=as.character(note_id)) %>% 
  dplyr::select(document, medical_specialty) %>% 
  dplyr::distinct()

specialty_gamma6 <- dplyr::left_join(specialty_gamma6, note_id_specialty_mapping6)
```

```{r}
specialty_gamma6 %>%
  dplyr::mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ medical_specialty) +
  ggplot2::labs(x = "topic", y = expression(gamma))
```



